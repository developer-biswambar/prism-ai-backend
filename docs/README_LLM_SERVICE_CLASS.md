# Pluggable LLM Service

This system provides a pluggable architecture for using different Large Language Model (LLM) providers in the transformation system.

## Current Support

âœ… **OpenAI** - Fully implemented  
ðŸš§ **Anthropic Claude** - Interface ready, implementation pending  
ðŸš§ **Google Gemini** - Interface ready, implementation pending  

## Quick Start

### Using OpenAI (Default)

1. Set environment variable:
```bash
export OPENAI_API_KEY="your-openai-api-key"
```

2. The system will automatically use OpenAI with GPT-4 model.

### Switching Providers

To switch to a different provider, set the `LLM_PROVIDER` environment variable:

```bash
# Use Anthropic Claude
export LLM_PROVIDER="anthropic"
export ANTHROPIC_API_KEY="your-anthropic-key"

# Use Google Gemini  
export LLM_PROVIDER="gemini"
export GOOGLE_API_KEY="your-google-key"
```

### Model Configuration

You can specify different models per provider:

```bash
# OpenAI models
export OPENAI_MODEL="gpt-3.5-turbo"  # or gpt-4, gpt-4-turbo, etc.

# Anthropic models
export ANTHROPIC_MODEL="claude-3-haiku-20240307"

# General model override (applies to current provider)
export LLM_MODEL="gpt-3.5-turbo"
```

## Adding New Providers

To add a new LLM provider:

1. **Create a new service class** in `llm_service.py`:

```python
class NewProviderLLMService(LLMServiceInterface):
    def __init__(self, api_key: str = None, model: str = "default-model"):
        self.api_key = api_key or os.getenv('NEW_PROVIDER_API_KEY')
        self.model = model
        # Initialize your provider's client here
    
    def generate_text(self, messages: List[LLMMessage], **kwargs) -> LLMResponse:
        # Implement the text generation logic
        pass
    
    def is_available(self) -> bool:
        # Check if the provider is configured and available
        return bool(self.api_key)
    
    def get_provider_name(self) -> str:
        return "new_provider"
    
    def get_model_name(self) -> str:
        return self.model
```

2. **Register the provider** in the factory:

```python
_providers = {
    "openai": OpenAILLMService,
    "anthropic": AnthropicLLMService,
    "gemini": GeminiLLMService,
    "new_provider": NewProviderLLMService,  # Add this line
}
```

3. **Add configuration** in `llm_config.py`:

```python
DEFAULT_MODELS = {
    "new_provider": "default-model-name",
    # ... other providers
}

# In get_provider_config method:
elif provider == "new_provider":
    config.update({
        "api_key": os.getenv('NEW_PROVIDER_API_KEY'),
        "temperature": float(os.getenv('NEW_PROVIDER_TEMPERATURE', '0.3')),
        "max_tokens": int(os.getenv('NEW_PROVIDER_MAX_TOKENS', '2000')),
    })

# In is_provider_configured method:
elif provider == "new_provider":
    return bool(config.get("api_key"))
```

## Environment Variables

### OpenAI
- `OPENAI_API_KEY` - Your OpenAI API key (required)
- `OPENAI_MODEL` - Model to use (default: gpt-4)
- `OPENAI_TEMPERATURE` - Temperature setting (default: 0.3)
- `OPENAI_MAX_TOKENS` - Max tokens (default: 2000)

### Anthropic
- `ANTHROPIC_API_KEY` - Your Anthropic API key
- `ANTHROPIC_MODEL` - Model to use (default: claude-3-sonnet-20240229)
- `ANTHROPIC_TEMPERATURE` - Temperature setting (default: 0.3)
- `ANTHROPIC_MAX_TOKENS` - Max tokens (default: 2000)

### Google Gemini
- `GOOGLE_API_KEY` - Your Google API key
- `GEMINI_MODEL` - Model to use (default: gemini-pro)
- `GEMINI_TEMPERATURE` - Temperature setting (default: 0.3)
- `GEMINI_MAX_TOKENS` - Max tokens (default: 2000)

### General
- `LLM_PROVIDER` - Which provider to use (default: openai)
- `LLM_MODEL` - Override model for any provider

## Health Check

Check which LLM provider is currently active:

```bash
curl http://localhost:8000/transformation/health
```

Response includes:
```json
{
  "status": "healthy",
  "service": "transformation",
  "llm_service": {
    "provider": "openai",
    "model": "gpt-4",
    "available": true
  }
}
```

## Programmatic Usage

```python
from app.services.llm_service import get_llm_service, LLMMessage

# Get the configured service
llm_service = get_llm_service()

# Create messages
messages = [
    LLMMessage(role="system", content="You are a helpful assistant."),
    LLMMessage(role="user", content="Generate a transformation config.")
]

# Generate response
response = llm_service.generate_text(messages, temperature=0.3)

if response.success:
    print(f"Generated by {response.provider} ({response.model}): {response.content}")
else:
    print(f"Error: {response.error}")
```

## Testing Different Providers

You can easily switch providers for testing:

```python
from app.services.llm_service import set_llm_service, LLMServiceFactory

# Test with different provider
anthropic_service = LLMServiceFactory.create_service("anthropic")
set_llm_service(anthropic_service)

# Now all AI generation will use Anthropic
```

## Benefits

- **Easy switching** between LLM providers
- **Consistent interface** across all providers
- **Fallback mechanisms** if a provider is unavailable
- **Environment-based configuration** 
- **Extensible architecture** for adding new providers
- **Testing flexibility** with provider swapping