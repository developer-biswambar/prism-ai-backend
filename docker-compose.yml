# Prism AI Backend - Standalone Docker Compose
# Production-ready deployment configuration

version: '3.8'

services:
  backend:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: prism-ai-backend
    restart: unless-stopped
    ports:
      - "${PORT:-8000}:8000"
    environment:
      # Application Configuration
      - HOST=0.0.0.0
      - PORT=8000
      - ENVIRONMENT=${ENVIRONMENT:-production}
      - DEBUG=${DEBUG:-false}
      - SECRET_KEY=${SECRET_KEY}
      - SERVER_URL=${SERVER_URL:-http://localhost:8000}
      
      # OpenAI Configuration
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - OPENAI_MODEL=${OPENAI_MODEL:-gpt-4-turbo}
      - OPENAI_MAX_TOKENS=${OPENAI_MAX_TOKENS:-4000}
      - OPENAI_TEMPERATURE=${OPENAI_TEMPERATURE:-0.1}
      
      # CORS Configuration (for frontend domains)
      - ALLOWED_ORIGINS=${ALLOWED_ORIGINS:-*}
      
      # File Processing Configuration
      - TEMP_DIR=/app/temp
      - BATCH_SIZE=${BATCH_SIZE:-100}
      - MAX_FILE_SIZE=${MAX_FILE_SIZE:-500}
      - MAX_ROWS_PER_FILE=${MAX_ROWS_PER_FILE:-1000000}
      - LARGE_FILE_THRESHOLD=${LARGE_FILE_THRESHOLD:-100000}
      
      # Performance Configuration
      - FTT_ML_CORES=${FTT_ML_CORES:-8}
      - MEMORY_LIMIT_GB=${MEMORY_LIMIT_GB:-16}
      - CHUNK_SIZE=${CHUNK_SIZE:-10000}
      
      # Logging Configuration
      - LOG_LEVEL=${LOG_LEVEL:-info}
      
      # Health Check Configuration
      - HEALTH_CHECK_TIMEOUT=${HEALTH_CHECK_TIMEOUT:-30}
      
    volumes:
      # Persistent temp directory for file processing
      - backend-temp:/app/temp
      # Optional: Mount for persistent logs
      - ./logs:/app/logs
      
    networks:
      - prism-ai-network
      
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
      
    deploy:
      resources:
        limits:
          cpus: '8'
          memory: 16G
        reservations:
          cpus: '4'
          memory: 8G

  # Alternative: Waitress server deployment
  backend-waitress:
    build:
      context: .
      dockerfile: Dockerfile.waitress
    container_name: prism-ai-backend-waitress
    restart: unless-stopped
    ports:
      - "${WAITRESS_PORT:-8001}:8000"
    environment:
      # Same environment as above, plus Waitress-specific
      - HOST=0.0.0.0
      - PORT=8000
      - ENVIRONMENT=${ENVIRONMENT:-production}
      - DEBUG=${DEBUG:-false}
      - SECRET_KEY=${SECRET_KEY}
      - SERVER_URL=${SERVER_URL:-http://localhost:8000}
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - OPENAI_MODEL=${OPENAI_MODEL:-gpt-4-turbo}
      - ALLOWED_ORIGINS=${ALLOWED_ORIGINS:-*}
      - TEMP_DIR=/app/temp
      - BATCH_SIZE=${BATCH_SIZE:-100}
      - MAX_FILE_SIZE=${MAX_FILE_SIZE:-500}
      - FTT_ML_CORES=${FTT_ML_CORES:-8}
      - LOG_LEVEL=${LOG_LEVEL:-info}
      
      # Waitress-specific configuration
      - WAITRESS_THREADS=${WAITRESS_THREADS:-8}
      - WAITRESS_CONNECTION_LIMIT=${WAITRESS_CONNECTION_LIMIT:-1000}
      - WAITRESS_CLEANUP_INTERVAL=${WAITRESS_CLEANUP_INTERVAL:-30}
      - WAITRESS_CHANNEL_TIMEOUT=${WAITRESS_CHANNEL_TIMEOUT:-120}
      
    volumes:
      - backend-temp:/app/temp
      - ./logs:/app/logs
      
    networks:
      - prism-ai-network
      
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
      
    deploy:
      resources:
        limits:
          cpus: '8'
          memory: 14G
        reservations:
          cpus: '4'
          memory: 6G
    
    # Comment out to disable Waitress service
    profiles:
      - waitress

  # Optional: Redis for caching (if needed)
  redis:
    image: redis:alpine
    container_name: prism-ai-redis
    restart: unless-stopped
    ports:
      - "${REDIS_PORT:-6379}:6379"
    volumes:
      - redis-data:/data
    networks:
      - prism-ai-network
    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 512M
    profiles:
      - redis

networks:
  prism-ai-network:
    driver: bridge

volumes:
  backend-temp:
    driver: local
  redis-data:
    driver: local